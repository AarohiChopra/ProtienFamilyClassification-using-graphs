{"cells":[{"cell_type":"markdown","source":["THIS WAS THE FILE THAT WAS USED INITIALLY WHEN WE WERE TESTING OUT GCN FOR OUR BIN DATA BUT THIS ISNT THE FINAL FILE FOR GCN, THE GCNonProtienGraphs FILE IS FINAL FILE. THIS FILE WAS INCLUDED TO SHOW OUR EXPERIMENTS"],"metadata":{"id":"ylT2RTqImvNS"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31845,"status":"ok","timestamp":1701231135166,"user":{"displayName":"AArohi Chopra","userId":"13635663229942628023"},"user_tz":480},"id":"L49vq9TpaYCo","outputId":"1acf2aed-cfda-4589-9b71-4a30b456a5e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/276/proj\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/276/proj"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nvGYcQtgcy_N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701231155958,"user_tz":480,"elapsed":17707,"user":{"displayName":"AArohi Chopra","userId":"13635663229942628023"}},"outputId":"3168aae8-e339-433b-b51c-6e95bc494780"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install dgl dglgo -f https://data.dgl.ai/wheels/repo.html -q"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7461,"status":"ok","timestamp":1701231165790,"user":{"displayName":"AArohi Chopra","userId":"13635663229942628023"},"user_tz":480},"id":"j_0PHv_KaW-L","outputId":"7f8b7273-3060-4398-f547-e156c06fdd06"},"outputs":[{"output_type":"stream","name":"stderr","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n"]},{"output_type":"stream","name":"stdout","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"]}],"source":["import dgl\n","from dgl.data.utils import split_dataset\n","\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import dgl.nn as dglnn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLZ-m_-PXNqp"},"outputs":[],"source":["# Load the graphs and the labels\n","graphs, label_dict = dgl.load_graphs('/content/drive/My Drive/276/proj/graphs/dgl_graphs.bin')\n","labels = label_dict['labels']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ego2o06Zc_c9"},"outputs":[],"source":["combined_dataset = list(zip(graphs, labels))\n","random.shuffle(combined_dataset)\n","split_index = int(len(combined_dataset) * 0.8)\n","trainset = combined_dataset[:split_index]\n","testset = combined_dataset[split_index:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lm4Kw6FdWTX"},"outputs":[],"source":["class GCN(nn.Module):\n","  def __init__(self, in_feats, hidden_size, num_classes):\n","    super(GCN, self).__init__()\n","    self.conv1 = dglnn.GraphConv(in_feats, hidden_size)\n","    self.conv2 = dglnn.GraphConv(hidden_size, num_classes)\n","\n","  def forward(self, g, features):\n","    x = F.relu(self.conv1(g, features))\n","    x = self.conv2(g, x)\n","    g.ndata['h'] = x\n","    return dgl.mean_nodes(g, 'h')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tab03oX-eMFV"},"outputs":[],"source":["def train(model, trainset, epochs, learning_rate):\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","  model.train()\n","  for epoch in range(epochs):\n","    total_loss = 0\n","    for graph, label in trainset:\n","      features = graph.ndata['b_factor'].float()\n","      features = features.unsqueeze(1)\n","      logits = model(graph, features)\n","      loss = F.cross_entropy(logits, label.view(-1))\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      total_loss += loss.item()\n","    avg_loss = total_loss / len(trainset)\n","    print(f\"Epoch {epoch}, Loss: {avg_loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xk-HmkVieN25"},"outputs":[],"source":["############s\n","def evaluate(model, testset):\n","  model.eval()\n","  with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for graph, label in testset:\n","      features = graph.ndata['b_factor'].float()\n","      features = features.unsqueeze(1)\n","\n","      logits = model(graph, features)\n","      _, predicted = torch.max(logits.data, 1)\n","      total += 1  # Increment the total count by 1 for each graph\n","      correct += (predicted == label).sum().item()\n","  accuracy = 100 * correct / total\n","  return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e8_Xpkttepbq"},"outputs":[],"source":["sample_graph = graphs[0]  # Take the first graph as a sample\n","sample_graph.ndata"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hdRZou4vktnt"},"outputs":[],"source":["class GCN(nn.Module):\n","  def __init__(self, in_feats, hidden_size, num_classes, dropout_rate):\n","    super(GCN, self).__init__()\n","    self.conv1 = dglnn.GraphConv(in_feats, hidden_size)\n","    self.conv2 = dglnn.GraphConv(hidden_size, hidden_size)  # Additional layer\n","    self.conv3 = dglnn.GraphConv(hidden_size, num_classes)\n","    self.dropout = nn.Dropout(p=0.5)  # Dropout layer\n","  def forward(self, g, features):\n","    x = F.relu(self.conv1(g, features))\n","    x = self.dropout(x)\n","    x = F.relu(self.conv2(g, x))\n","    x = self.dropout(x)\n","    x = self.conv3(g, x)\n","    g.ndata['h'] = x\n","    return dgl.mean_nodes(g, 'h')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":204,"status":"ok","timestamp":1701220810357,"user":{"displayName":"Daniel Quintana","userId":"16118601250046003180"},"user_tz":480},"id":"0A0qThIJeQLs","outputId":"87bb3b8a-4970-4ca8-d0d5-1b75c8bba664"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n# Initialize the model\\nmodel = GCN(in_feats=1, hidden_size=256, num_classes=10)\\n\\n# Train the model with a smaller learning rate and more epochs\\ntrain(model, trainset, epochs=10, learning_rate=0.0005)\\n\\n\\n# Evaluate the model\\naccuracy = evaluate(model, testset)\\nprint(f\"Test accuracy: {accuracy}%\")\\n'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","model = GCN(in_feats=1, hidden_size=256, num_classes=10)\n","train(model, trainset, epochs=10, learning_rate=0.0005)\n","accuracy = evaluate(model, testset)\n","print(f\"Test accuracy: {accuracy}%\")\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":80198,"status":"error","timestamp":1701221611186,"user":{"displayName":"Daniel Quintana","userId":"16118601250046003180"},"user_tz":480},"id":"c74UyE3fgdEB","outputId":"2d51091e-8a1a-4469-9827-720ea7f29d07"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Loss: 2.4080974222125864\n","Epoch 1, Loss: 2.3033141761196587\n","Epoch 2, Loss: 2.29706900505195\n","Epoch 3, Loss: 2.2967570938711477\n","Epoch 4, Loss: 2.2928391948230287\n","Epoch 5, Loss: 2.2839655247995427\n","Epoch 6, Loss: 2.2926002368144225\n","Epoch 7, Loss: 2.287236283895067\n","Epoch 8, Loss: 2.278917673611103\n","Epoch 9, Loss: 2.2750260602860224\n","LR: 0.01, Hidden Size: 16, Epochs: 10, Dropout: 0.3, Accuracy: 9.0%\n","Epoch 0, Loss: 2.4155713040428353\n","Epoch 1, Loss: 2.3211523521514166\n","Epoch 2, Loss: 2.3065444244477984\n","Epoch 3, Loss: 2.294940422725558\n","Epoch 4, Loss: 2.292730655362433\n","Epoch 5, Loss: 2.2779390229318377\n","Epoch 6, Loss: 2.2749097991109193\n","Epoch 7, Loss: 2.278762902532305\n","Epoch 8, Loss: 2.278726803255559\n","Epoch 9, Loss: 2.2814388754299113\n","LR: 0.01, Hidden Size: 16, Epochs: 10, Dropout: 0.5, Accuracy: 9.0%\n","Epoch 0, Loss: 2.6422569824937607\n","Epoch 1, Loss: 2.3112335021334482\n","Epoch 2, Loss: 2.3115309957872356\n","Epoch 3, Loss: 2.307743164456279\n","Epoch 4, Loss: 2.2991793515688195\n","Epoch 5, Loss: 2.3010083121763434\n","Epoch 6, Loss: 2.295801564117422\n","Epoch 7, Loss: 2.283451171596546\n","Epoch 8, Loss: 2.28959813348034\n","Epoch 9, Loss: 2.2833062741242554\n","LR: 0.01, Hidden Size: 16, Epochs: 10, Dropout: 0.7, Accuracy: 9.0%\n","Epoch 0, Loss: 2.4333890232077815\n","Epoch 1, Loss: 2.3227818651605667\n","Epoch 2, Loss: 2.2983110381248304\n","Epoch 3, Loss: 2.2922944677503487\n","Epoch 4, Loss: 2.291546660855898\n","Epoch 5, Loss: 2.290674707047025\n","Epoch 6, Loss: 2.2907477657597766\n","Epoch 7, Loss: 2.289231224317001\n","Epoch 8, Loss: 2.2975552513784634\n","Epoch 9, Loss: 2.2719813728691043\n","Epoch 10, Loss: 2.2908144853915786\n","Epoch 11, Loss: 2.282831728757175\n","Epoch 12, Loss: 2.27626035588427\n","Epoch 13, Loss: 2.2816484793086995\n","Epoch 14, Loss: 2.281717855754054\n","Epoch 15, Loss: 2.289641640090703\n","Epoch 16, Loss: 2.288070580490251\n","Epoch 17, Loss: 2.276463865038745\n","Epoch 18, Loss: 2.2834689121198535\n","Epoch 19, Loss: 2.269434316975431\n","LR: 0.01, Hidden Size: 16, Epochs: 20, Dropout: 0.3, Accuracy: 9.0%\n","Epoch 0, Loss: 2.8243548881103817\n","Epoch 1, Loss: 2.3042012323114207\n","Epoch 2, Loss: 2.3319297324595296\n","Epoch 3, Loss: 2.2998941356974436\n","Epoch 4, Loss: 2.2940993222377655\n","Epoch 5, Loss: 2.2668150215967557\n","Epoch 6, Loss: 2.285457530266659\n","Epoch 7, Loss: 2.2844301793434867\n","Epoch 8, Loss: 2.281102905297339\n","Epoch 9, Loss: 2.2816939439092363\n","Epoch 10, Loss: 2.2811073949910643\n","Epoch 11, Loss: 2.2836771365395165\n","Epoch 12, Loss: 2.2714907609878625\n","Epoch 13, Loss: 2.284868406547341\n","Epoch 14, Loss: 2.2707841163664533\n","Epoch 15, Loss: 2.270465737745577\n","Epoch 16, Loss: 2.27877316170169\n","Epoch 17, Loss: 2.2823911806694546\n","Epoch 18, Loss: 2.280294544266579\n","Epoch 19, Loss: 2.285599188398299\n","LR: 0.01, Hidden Size: 16, Epochs: 20, Dropout: 0.5, Accuracy: 9.0%\n","Epoch 0, Loss: 2.4418855184189656\n","Epoch 1, Loss: 2.305400296784284\n","Epoch 2, Loss: 2.3026957047314274\n","Epoch 3, Loss: 2.3099903814531957\n","Epoch 4, Loss: 2.297081849180666\n","Epoch 5, Loss: 2.297541979336201\n","Epoch 6, Loss: 2.282968995863931\n","Epoch 7, Loss: 2.277598019792024\n","Epoch 8, Loss: 2.2885997889652137\n","Epoch 9, Loss: 2.2856907574343204\n","Epoch 10, Loss: 2.2836682692655645\n","Epoch 11, Loss: 2.2790317384521463\n","Epoch 12, Loss: 2.2697242785777365\n","Epoch 13, Loss: 2.2776091667942535\n","Epoch 14, Loss: 2.2798296074221907\n","Epoch 15, Loss: 2.2755790764097132\n","Epoch 16, Loss: 2.2838790733413887\n","Epoch 17, Loss: 2.282316800197563\n","Epoch 18, Loss: 2.2854764921623363\n","Epoch 19, Loss: 2.275429413730937\n","LR: 0.01, Hidden Size: 16, Epochs: 20, Dropout: 0.7, Accuracy: 9.0%\n","Epoch 0, Loss: 2.6411603575879896\n","Epoch 1, Loss: 2.3078677297236028\n","Epoch 2, Loss: 2.3004014182807806\n","Epoch 3, Loss: 2.2952528195572377\n","Epoch 4, Loss: 2.306081914886795\n","Epoch 5, Loss: 2.290537078727158\n","Epoch 6, Loss: 2.291106425729909\n","Epoch 7, Loss: 2.286504740852461\n","Epoch 8, Loss: 2.2798471398520888\n","Epoch 9, Loss: 2.27645853460582\n","Epoch 10, Loss: 2.287325054183042\n","Epoch 11, Loss: 2.2803393364848947\n","Epoch 12, Loss: 2.2792319674838457\n","Epoch 13, Loss: 2.2711085842964343\n","Epoch 14, Loss: 2.2782360496078815\n","Epoch 15, Loss: 2.2826349681481384\n","Epoch 16, Loss: 2.2808677194112525\n","Epoch 17, Loss: 2.2832885911888945\n","Epoch 18, Loss: 2.274524117528197\n","Epoch 19, Loss: 2.279901470531497\n","Epoch 20, Loss: 2.2764073451957607\n","Epoch 21, Loss: 2.2721477070249114\n","Epoch 22, Loss: 2.2735526966570614\n","Epoch 23, Loss: 2.282241192228513\n","Epoch 24, Loss: 2.290328073919865\n","Epoch 25, Loss: 2.2757992025903593\n","Epoch 26, Loss: 2.2827021446741913\n","Epoch 27, Loss: 2.280058451314319\n","Epoch 28, Loss: 2.2768309379281257\n","Epoch 29, Loss: 2.279738430391278\n","LR: 0.01, Hidden Size: 16, Epochs: 30, Dropout: 0.3, Accuracy: 9.0%\n","Epoch 0, Loss: 2.4499342606442616\n","Epoch 1, Loss: 2.287326542431848\n","Epoch 2, Loss: 2.281292404820746\n","Epoch 3, Loss: 2.293784500568881\n","Epoch 4, Loss: 2.282442415119114\n","Epoch 5, Loss: 2.2808527519230855\n","Epoch 6, Loss: 2.278502365102744\n","Epoch 7, Loss: 2.2905815279573427\n","Epoch 8, Loss: 2.2804222111415147\n","Epoch 9, Loss: 2.273059400251336\n","Epoch 10, Loss: 2.279702986093392\n","Epoch 11, Loss: 2.277392867812537\n","Epoch 12, Loss: 2.2739638670494684\n","Epoch 13, Loss: 2.280537182451191\n","Epoch 14, Loss: 2.279331641052301\n","Epoch 15, Loss: 2.262917585390851\n","Epoch 16, Loss: 2.274878298504311\n","Epoch 17, Loss: 2.2675518253841496\n","Epoch 18, Loss: 2.256991809248028\n","Epoch 19, Loss: 2.2705045837656894\n","Epoch 20, Loss: 2.273015149777993\n","Epoch 21, Loss: 2.2665852981253076\n","Epoch 22, Loss: 2.279349986771892\n","Epoch 23, Loss: 2.2876524264202978\n","Epoch 24, Loss: 2.2715613974963214\n","Epoch 25, Loss: 2.276622021706182\n","Epoch 26, Loss: 2.269043109097278\n","Epoch 27, Loss: 2.2746841890321936\n","Epoch 28, Loss: 2.2811293723738584\n","Epoch 29, Loss: 2.2703190619933573\n","LR: 0.01, Hidden Size: 16, Epochs: 30, Dropout: 0.5, Accuracy: 9.0%\n","Epoch 0, Loss: 2.323726907111051\n","Epoch 1, Loss: 2.3125939694861124\n","Epoch 2, Loss: 2.3126333470930134\n","Epoch 3, Loss: 2.3126381277141714\n","Epoch 4, Loss: 2.3126390371705057\n","Epoch 5, Loss: 2.3126393487877714\n","Epoch 6, Loss: 2.312639456345025\n","Epoch 7, Loss: 2.312639522373228\n","Epoch 8, Loss: 2.312639511617503\n","Epoch 9, Loss: 2.3126395256596997\n","Epoch 10, Loss: 2.3126395755543148\n","Epoch 11, Loss: 2.3126395922854432\n","Epoch 12, Loss: 2.3126395692801416\n","Epoch 13, Loss: 2.3126395579268757\n","Epoch 14, Loss: 2.312639578243246\n","Epoch 15, Loss: 2.312639594078064\n","Epoch 16, Loss: 2.3126395650973595\n","Epoch 17, Loss: 2.3126395513539326\n","Epoch 18, Loss: 2.3126395764506253\n","Epoch 19, Loss: 2.3126395636035086\n","Epoch 20, Loss: 2.3126395172941354\n","Epoch 21, Loss: 2.3126395636035086\n","Epoch 22, Loss: 2.3126395656948997\n","Epoch 23, Loss: 2.3126395477686907\n","Epoch 24, Loss: 2.312639520281837\n","Epoch 25, Loss: 2.3126395364154253\n","Epoch 26, Loss: 2.3126395665912103\n","Epoch 27, Loss: 2.312639540299437\n","Epoch 28, Loss: 2.3126395253609298\n","Epoch 29, Loss: 2.3126395331289533\n","LR: 0.01, Hidden Size: 16, Epochs: 30, Dropout: 0.7, Accuracy: 9.0%\n","Epoch 0, Loss: 2.4826500427379345\n","Epoch 1, Loss: 2.317390962053361\n","Epoch 2, Loss: 2.31416903305472\n","Epoch 3, Loss: 2.3127563577845582\n","Epoch 4, Loss: 2.3065347754417505\n","Epoch 5, Loss: 2.292667857835765\n","Epoch 6, Loss: 2.2858271673508455\n","Epoch 7, Loss: 2.281967734334463\n","Epoch 8, Loss: 2.277280064453756\n","Epoch 9, Loss: 2.2829042262302006\n","LR: 0.01, Hidden Size: 32, Epochs: 10, Dropout: 0.3, Accuracy: 9.0%\n","Epoch 0, Loss: 2.4046485990771376\n","Epoch 1, Loss: 2.3091769489578735\n","Epoch 2, Loss: 2.291338693079793\n","Epoch 3, Loss: 2.2835694240000013\n","Epoch 4, Loss: 2.2785354326094005\n","Epoch 5, Loss: 2.280702587058371\n","Epoch 6, Loss: 2.2840069091708437\n","Epoch 7, Loss: 2.271377465405261\n","Epoch 8, Loss: 2.284833856393819\n","Epoch 9, Loss: 2.280610528356748\n","LR: 0.01, Hidden Size: 32, Epochs: 10, Dropout: 0.5, Accuracy: 9.0%\n","Epoch 0, Loss: 2.4411423759427584\n","Epoch 1, Loss: 2.32015569855396\n","Epoch 2, Loss: 2.2983821615539397\n","Epoch 3, Loss: 2.2986877857891836\n","Epoch 4, Loss: 2.282038432763035\n","Epoch 5, Loss: 2.2812798498268414\n","Epoch 6, Loss: 2.281481991435651\n","Epoch 7, Loss: 2.2759690865836943\n","Epoch 8, Loss: 2.2846182345746455\n","Epoch 9, Loss: 2.2850810488065085\n","LR: 0.01, Hidden Size: 32, Epochs: 10, Dropout: 0.7, Accuracy: 9.0%\n","Epoch 0, Loss: 2.5369916252026283\n","Epoch 1, Loss: 2.343482118352015\n","Epoch 2, Loss: 2.3085452156855646\n","Epoch 3, Loss: 2.3013477278383156\n","Epoch 4, Loss: 2.285033465029303\n","Epoch 5, Loss: 2.285333172121741\n","Epoch 6, Loss: 2.2835264722431514\n","Epoch 7, Loss: 2.2874225790339304\n","Epoch 8, Loss: 2.2749457143452534\n","Epoch 9, Loss: 2.2891440793386377\n","Epoch 10, Loss: 2.2713831971611893\n","Epoch 11, Loss: 2.271360285860255\n","Epoch 12, Loss: 2.2882721201519023\n","Epoch 13, Loss: 2.2795966983887186\n","Epoch 14, Loss: 2.27254331171662\n","Epoch 15, Loss: 2.273320971799076\n","Epoch 16, Loss: 2.275679582641238\n","Epoch 17, Loss: 2.281029895134737\n","Epoch 18, Loss: 2.282654998147099\n","Epoch 19, Loss: 2.266136438177343\n","LR: 0.01, Hidden Size: 32, Epochs: 20, Dropout: 0.3, Accuracy: 9.0%\n","Epoch 0, Loss: 2.6481512892515138\n","Epoch 1, Loss: 2.3451111150134056\n","Epoch 2, Loss: 2.3121609067857114\n","Epoch 3, Loss: 2.306469038016814\n","Epoch 4, Loss: 2.305450296939764\n","Epoch 5, Loss: 2.3010224702961763\n","Epoch 6, Loss: 2.292355072976354\n","Epoch 7, Loss: 2.28939290856359\n","Epoch 8, Loss: 2.293727984553889\n","Epoch 9, Loss: 2.2869670271574702\n","Epoch 10, Loss: 2.29173210315537\n","Epoch 11, Loss: 2.2811453376796313\n","Epoch 12, Loss: 2.2862870351114966\n","Epoch 13, Loss: 2.2715606826887393\n","Epoch 14, Loss: 2.292792182817196\n","Epoch 15, Loss: 2.281654933117386\n","Epoch 16, Loss: 2.2817798083587397\n","Epoch 17, Loss: 2.269782020371958\n","Epoch 18, Loss: 2.280263745993899\n","Epoch 19, Loss: 2.282101879442545\n","LR: 0.01, Hidden Size: 32, Epochs: 20, Dropout: 0.5, Accuracy: 9.0%\n","Epoch 0, Loss: 2.3801953900129273\n","Epoch 1, Loss: 2.318835170122615\n","Epoch 2, Loss: 2.310145712585975\n","Epoch 3, Loss: 2.3046918831074747\n","Epoch 4, Loss: 2.2959325451002384\n","Epoch 5, Loss: 2.299894816594614\n","Epoch 6, Loss: 2.2902177072930754\n","Epoch 7, Loss: 2.289947168868885\n","Epoch 8, Loss: 2.2921482341032577\n","Epoch 9, Loss: 2.2913239973977695\n","Epoch 10, Loss: 2.2777306268687236\n","Epoch 11, Loss: 2.28919554025607\n","Epoch 12, Loss: 2.281772083955301\n","Epoch 13, Loss: 2.274763071895542\n","Epoch 14, Loss: 2.2746106134470843\n","Epoch 15, Loss: 2.269465702667571\n","Epoch 16, Loss: 2.2705408993893696\n","Epoch 17, Loss: 2.279405194416381\n","Epoch 18, Loss: 2.2762622323101924\n","Epoch 19, Loss: 2.2695663184450385\n","LR: 0.01, Hidden Size: 32, Epochs: 20, Dropout: 0.7, Accuracy: 9.0%\n","Epoch 0, Loss: 2.4953989627943804\n","Epoch 1, Loss: 2.308084993508824\n","Epoch 2, Loss: 2.3056805501306865\n","Epoch 3, Loss: 2.3159889327852348\n","Epoch 4, Loss: 2.2974987454282907\n","Epoch 5, Loss: 2.2969471864533006\n","Epoch 6, Loss: 2.28902153338406\n","Epoch 7, Loss: 2.2765935009583496\n","Epoch 8, Loss: 2.290210099596726\n","Epoch 9, Loss: 2.2838088279977478\n","Epoch 10, Loss: 2.2694857606762335\n","Epoch 11, Loss: 2.2690032763140544\n","Epoch 12, Loss: 2.2742660891516766\n","Epoch 13, Loss: 2.2727246021940593\n","Epoch 14, Loss: 2.2760086394753074\n","Epoch 15, Loss: 2.280388156871748\n","Epoch 16, Loss: 2.264107115062556\n","Epoch 17, Loss: 2.2917082258931973\n","Epoch 18, Loss: 2.270245573873209\n","Epoch 19, Loss: 2.2809430246514486\n","Epoch 20, Loss: 2.264872315533478\n","Epoch 21, Loss: 2.2680565585991492\n","Epoch 22, Loss: 2.2809293096824397\n","Epoch 23, Loss: 2.279807451972388\n","Epoch 24, Loss: 2.2712090928154183\n","Epoch 25, Loss: 2.283096952844682\n","Epoch 26, Loss: 2.274442372054683\n","Epoch 27, Loss: 2.268043234010687\n","Epoch 28, Loss: 2.287333331908797\n","Epoch 29, Loss: 2.2801966768757143\n","LR: 0.01, Hidden Size: 32, Epochs: 30, Dropout: 0.3, Accuracy: 9.0%\n","Epoch 0, Loss: 2.442112404594016\n","Epoch 1, Loss: 2.312536569167498\n","Epoch 2, Loss: 2.2802766017746507\n","Epoch 3, Loss: 2.298507932134738\n","Epoch 4, Loss: 2.284184628411343\n","Epoch 5, Loss: 2.2858928188046717\n","Epoch 6, Loss: 2.285268369623294\n","Epoch 7, Loss: 2.283721211111934\n","Epoch 8, Loss: 2.2820884349352135\n","Epoch 9, Loss: 2.283943137728182\n","Epoch 10, Loss: 2.28952863147683\n","Epoch 11, Loss: 2.2834331806291615\n","Epoch 12, Loss: 2.284897489729979\n","Epoch 13, Loss: 2.282096068960682\n","Epoch 14, Loss: 2.28692598958362\n","Epoch 15, Loss: 2.2840688944161687\n","Epoch 16, Loss: 2.2894860731629203\n","Epoch 17, Loss: 2.2894657246749803\n","Epoch 18, Loss: 2.2761927290369095\n","Epoch 19, Loss: 2.283447007636976\n","Epoch 20, Loss: 2.274282182293727\n","Epoch 21, Loss: 2.285918113731203\n","Epoch 22, Loss: 2.2777721366487946\n","Epoch 23, Loss: 2.278259291834102\n","Epoch 24, Loss: 2.2868886858150175\n","Epoch 25, Loss: 2.276267468257058\n","Epoch 26, Loss: 2.27292137396963\n","Epoch 27, Loss: 2.284125829176198\n","Epoch 28, Loss: 2.2891519080128586\n","Epoch 29, Loss: 2.2862073949405124\n","LR: 0.01, Hidden Size: 32, Epochs: 30, Dropout: 0.5, Accuracy: 9.0%\n","Epoch 0, Loss: 2.553065295132778\n","Epoch 1, Loss: 2.3370508509769774\n","Epoch 2, Loss: 2.3055889187450695\n","Epoch 3, Loss: 2.2909413242997383\n","Epoch 4, Loss: 2.285333346603508\n","Epoch 5, Loss: 2.291200551920965\n","Epoch 6, Loss: 2.2823698009763445\n","Epoch 7, Loss: 2.2828598739509296\n","Epoch 8, Loss: 2.286688597578751\n","Epoch 9, Loss: 2.2888264918984627\n","Epoch 10, Loss: 2.2879160370743064\n","Epoch 11, Loss: 2.2843929520226958\n","Epoch 12, Loss: 2.284882927971675\n","Epoch 13, Loss: 2.2904293816489982\n","Epoch 14, Loss: 2.2846976866698205\n","Epoch 15, Loss: 2.2810776874535064\n","Epoch 16, Loss: 2.291011125372167\n","Epoch 17, Loss: 2.2892168658717833\n","Epoch 18, Loss: 2.2788848076249124\n","Epoch 19, Loss: 2.283809881461294\n","Epoch 20, Loss: 2.2876092295598864\n","Epoch 21, Loss: 2.290678155452088\n","Epoch 22, Loss: 2.294677594102415\n","Epoch 23, Loss: 2.2909985867359284\n","Epoch 24, Loss: 2.2938562589779234\n","Epoch 25, Loss: 2.2949539791970026\n","Epoch 26, Loss: 2.2876915519398855\n","Epoch 27, Loss: 2.2807664236329255\n","Epoch 28, Loss: 2.2937136262580564\n","Epoch 29, Loss: 2.286309513158368\n","LR: 0.01, Hidden Size: 32, Epochs: 30, Dropout: 0.7, Accuracy: 9.0%\n","Epoch 0, Loss: 2.5697994281848273\n","Epoch 1, Loss: 2.3479673100593397\n","Epoch 2, Loss: 2.318544186296917\n","Epoch 3, Loss: 2.313373673679237\n","Epoch 4, Loss: 2.3032917992811752\n","Epoch 5, Loss: 2.2960159128769897\n","Epoch 6, Loss: 2.29219248838592\n","Epoch 7, Loss: 2.2906494438648224\n","Epoch 8, Loss: 2.287805910397293\n","Epoch 9, Loss: 2.2770559736959317\n","LR: 0.01, Hidden Size: 64, Epochs: 10, Dropout: 0.3, Accuracy: 9.0%\n","Epoch 0, Loss: 2.4744390531403377\n","Epoch 1, Loss: 2.3374874395759484\n","Epoch 2, Loss: 2.309724293257061\n","Epoch 3, Loss: 2.293562773026918\n","Epoch 4, Loss: 2.285377774172857\n","Epoch 5, Loss: 2.2865683151068246\n","Epoch 6, Loss: 2.2908523686248854\n","Epoch 7, Loss: 2.2854424177255845\n","Epoch 8, Loss: 2.2937480750538053\n","Epoch 9, Loss: 2.2838307728145955\n","LR: 0.01, Hidden Size: 64, Epochs: 10, Dropout: 0.5, Accuracy: 9.0%\n","Epoch 0, Loss: 2.4803269471142224\n","Epoch 1, Loss: 2.3466418794820783\n","Epoch 2, Loss: 2.2869906267127895\n","Epoch 3, Loss: 2.2847100650905667\n","Epoch 4, Loss: 2.284623502788687\n","Epoch 5, Loss: 2.270174111713443\n","Epoch 6, Loss: 2.279712866347535\n","Epoch 7, Loss: 2.2766691175917337\n","Epoch 8, Loss: 2.2802279348062693\n","Epoch 9, Loss: 2.2777378049709442\n","LR: 0.01, Hidden Size: 64, Epochs: 10, Dropout: 0.7, Accuracy: 9.0%\n","Epoch 0, Loss: 2.5380132446126233\n","Epoch 1, Loss: 2.325680099558412\n","Epoch 2, Loss: 2.317744323185512\n","Epoch 3, Loss: 2.2995976517373755\n","Epoch 4, Loss: 2.290160804090643\n","Epoch 5, Loss: 2.288851428226122\n","Epoch 6, Loss: 2.282255644039403\n","Epoch 7, Loss: 2.286043172043965\n","Epoch 8, Loss: 2.2775803459318062\n","Epoch 9, Loss: 2.2691331664870558\n","Epoch 10, Loss: 2.2727470480857934\n","Epoch 11, Loss: 2.2820977395339717\n","Epoch 12, Loss: 2.277397741575289\n","Epoch 13, Loss: 2.273924563911027\n","Epoch 14, Loss: 2.2656906864099335\n","Epoch 15, Loss: 2.2624525399435136\n","Epoch 16, Loss: 2.266377455235125\n","Epoch 17, Loss: 2.2719664451173673\n","Epoch 18, Loss: 2.27191271675858\n","Epoch 19, Loss: 2.2752531676513508\n","LR: 0.01, Hidden Size: 64, Epochs: 20, Dropout: 0.3, Accuracy: 9.0%\n","Epoch 0, Loss: 2.500679968069669\n","Epoch 1, Loss: 2.3212102119785203\n","Epoch 2, Loss: 2.3485535118812906\n","Epoch 3, Loss: 2.320344375488453\n","Epoch 4, Loss: 2.3012880230308475\n","Epoch 5, Loss: 2.3066489351423165\n","Epoch 6, Loss: 2.3006218422325633\n","Epoch 7, Loss: 2.2887885537661408\n","Epoch 8, Loss: 2.284784900664088\n","Epoch 9, Loss: 2.3088033239644274\n","Epoch 10, Loss: 2.291999582733427\n","Epoch 11, Loss: 2.2950465653773238\n","Epoch 12, Loss: 2.291008132890352\n","Epoch 13, Loss: 2.2724410077757704\n","Epoch 14, Loss: 2.2803555297896376\n","Epoch 15, Loss: 2.283450656516809\n","Epoch 16, Loss: 2.28226088297397\n","Epoch 17, Loss: 2.272535426051993\n","Epoch 18, Loss: 2.300671814826497\n","Epoch 19, Loss: 2.268531862394254\n","LR: 0.01, Hidden Size: 64, Epochs: 20, Dropout: 0.5, Accuracy: 9.0%\n","Epoch 0, Loss: 2.46493069353259\n","Epoch 1, Loss: 2.3275008110474227\n","Epoch 2, Loss: 2.3202448671772364\n","Epoch 3, Loss: 2.320813237873833\n","Epoch 4, Loss: 2.301666319146192\n","Epoch 5, Loss: 2.291408568097834\n","Epoch 6, Loss: 2.291043874613922\n","Epoch 7, Loss: 2.287733638645115\n","Epoch 8, Loss: 2.2775900925609998\n","Epoch 9, Loss: 2.296409645474943\n","Epoch 10, Loss: 2.286016004575524\n","Epoch 11, Loss: 2.2901771013300523\n","Epoch 12, Loss: 2.289508070413929\n","Epoch 13, Loss: 2.2757648062287714\n","Epoch 14, Loss: 2.290072640978304\n","Epoch 15, Loss: 2.286729384036291\n","Epoch 16, Loss: 2.2873271940495434\n","Epoch 17, Loss: 2.290238378938278\n","Epoch 18, Loss: 2.283947415817949\n","Epoch 19, Loss: 2.2943723195776307\n","LR: 0.01, Hidden Size: 64, Epochs: 20, Dropout: 0.7, Accuracy: 9.0%\n","Epoch 0, Loss: 2.5225236604648424\n","Epoch 1, Loss: 2.379503737416184\n","Epoch 2, Loss: 2.3121339958114433\n","Epoch 3, Loss: 2.3215899654199603\n","Epoch 4, Loss: 2.316536151974422\n","Epoch 5, Loss: 2.30551301611396\n","Epoch 6, Loss: 2.295381086736096\n","Epoch 7, Loss: 2.2862608447708284\n","Epoch 8, Loss: 2.2764023192096174\n","Epoch 9, Loss: 2.2879348646728017\n","Epoch 10, Loss: 2.2797943230410267\n","Epoch 11, Loss: 2.284360577139938\n","Epoch 12, Loss: 2.2734052175866033\n","Epoch 13, Loss: 2.2777601059965025\n","Epoch 14, Loss: 2.2817968572291516\n","Epoch 15, Loss: 2.2699083516322878\n","Epoch 16, Loss: 2.2731337061053827\n","Epoch 17, Loss: 2.272985242139128\n","Epoch 18, Loss: 2.2803962314338015\n","Epoch 19, Loss: 2.269171489061867\n","Epoch 20, Loss: 2.27052602941231\n","Epoch 21, Loss: 2.2703375100790706\n","Epoch 22, Loss: 2.26861084837065\n","Epoch 23, Loss: 2.2695341587514806\n","Epoch 24, Loss: 2.2649067934593163\n","Epoch 25, Loss: 2.258433063899664\n","Epoch 26, Loss: 2.280514069891215\n","Epoch 27, Loss: 2.284076074161625\n","Epoch 28, Loss: 2.2739790226881365\n","Epoch 29, Loss: 2.2689945807582452\n","LR: 0.01, Hidden Size: 64, Epochs: 30, Dropout: 0.3, Accuracy: 9.0%\n","Epoch 0, Loss: 2.465486458005538\n","Epoch 1, Loss: 2.3462800077328407\n","Epoch 2, Loss: 2.324456514272475\n","Epoch 3, Loss: 2.312134797561139\n","Epoch 4, Loss: 2.312632093154697\n","Epoch 5, Loss: 2.3126324325575864\n","Epoch 6, Loss: 2.3126378110178134\n","Epoch 7, Loss: 2.312638859999807\n","Epoch 8, Loss: 2.3126392726013836\n","Epoch 9, Loss: 2.3126394324434134\n","Epoch 10, Loss: 2.3126394763626252\n","Epoch 11, Loss: 2.31263948592327\n","Epoch 12, Loss: 2.312639536714195\n","Epoch 13, Loss: 2.312639559719497\n","Epoch 14, Loss: 2.3126395552379444\n","Epoch 15, Loss: 2.312639499965467\n","Epoch 16, Loss: 2.312639532232643\n","Epoch 17, Loss: 2.3126395656948997\n","Epoch 18, Loss: 2.3126395627071985\n","Epoch 19, Loss: 2.3126395331289533\n","Epoch 20, Loss: 2.3126395373117354\n","Epoch 21, Loss: 2.3126395612133477\n","Epoch 22, Loss: 2.312639515800285\n","Epoch 23, Loss: 2.3126395152027444\n","Epoch 24, Loss: 2.3126395343240342\n","Epoch 25, Loss: 2.3126395447809895\n","Epoch 26, Loss: 2.312639551951473\n","Epoch 27, Loss: 2.3126395152027444\n","Epoch 28, Loss: 2.3126395588231863\n","Epoch 29, Loss: 2.3126395872063505\n","LR: 0.01, Hidden Size: 64, Epochs: 30, Dropout: 0.5, Accuracy: 9.0%\n","Epoch 0, Loss: 2.5310423861667375\n","Epoch 1, Loss: 2.321699015329356\n","Epoch 2, Loss: 2.3311521039906897\n","Epoch 3, Loss: 2.340369089205462\n","Epoch 4, Loss: 2.3061470142880776\n","Epoch 5, Loss: 2.2885411079963647\n","Epoch 6, Loss: 2.294795182563906\n","Epoch 7, Loss: 2.293141665464655\n","Epoch 8, Loss: 2.2825774072405687\n","Epoch 9, Loss: 2.286269215264715\n","Epoch 10, Loss: 2.287365994656594\n","Epoch 11, Loss: 2.280285695142913\n","Epoch 12, Loss: 2.279083059694534\n","Epoch 13, Loss: 2.2873039871528933\n","Epoch 14, Loss: 2.279350771641074\n","Epoch 15, Loss: 2.281686630464138\n","Epoch 16, Loss: 2.2748910963983464\n","Epoch 17, Loss: 2.27916105169999\n","Epoch 18, Loss: 2.272155805637962\n","Epoch 19, Loss: 2.282003652631191\n","Epoch 20, Loss: 2.263721382408811\n","Epoch 21, Loss: 2.282008982391883\n","Epoch 22, Loss: 2.2786253088697754\n","Epoch 23, Loss: 2.2666499356279397\n","Epoch 24, Loss: 2.267891868762205\n","Epoch 25, Loss: 2.2612823590001367\n","Epoch 26, Loss: 2.2699267239797685\n","Epoch 27, Loss: 2.2793882805900765\n","Epoch 28, Loss: 2.2733658898743174\n","Epoch 29, Loss: 2.2772354480616728\n","LR: 0.01, Hidden Size: 64, Epochs: 30, Dropout: 0.7, Accuracy: 9.0%\n","Epoch 0, Loss: 2.513522267117536\n","Epoch 1, Loss: 2.2953077326143596\n","Epoch 2, Loss: 2.288591261122161\n","Epoch 3, Loss: 2.2874747723863837\n","Epoch 4, Loss: 2.2820857180688616\n","Epoch 5, Loss: 2.280261298767606\n","Epoch 6, Loss: 2.2778161724557853\n","Epoch 7, Loss: 2.273673980307758\n","Epoch 8, Loss: 2.2717401578760983\n","Epoch 9, Loss: 2.2705294864070145\n","LR: 0.001, Hidden Size: 16, Epochs: 10, Dropout: 0.3, Accuracy: 8.5%\n","Epoch 0, Loss: 3.1856625582042493\n","Epoch 1, Loss: 2.3607335585101805\n","Epoch 2, Loss: 2.3174363165570977\n","Epoch 3, Loss: 2.295314681634568\n","Epoch 4, Loss: 2.274235509093244\n","Epoch 5, Loss: 2.2732609449920798\n","Epoch 6, Loss: 2.2641414907343105\n","Epoch 7, Loss: 2.2667842788206305\n","Epoch 8, Loss: 2.2536146340513588\n","Epoch 9, Loss: 2.266781552617711\n","LR: 0.001, Hidden Size: 16, Epochs: 10, Dropout: 0.5, Accuracy: 12.0%\n","Epoch 0, Loss: 3.393101185798309\n","Epoch 1, Loss: 2.4288430312895835\n","Epoch 2, Loss: 2.304797712498739\n","Epoch 3, Loss: 2.298326563416866\n","Epoch 4, Loss: 2.288040079045714\n","Epoch 5, Loss: 2.2836305004612245\n","Epoch 6, Loss: 2.2829525710496688\n","Epoch 7, Loss: 2.2711666941194606\n","Epoch 8, Loss: 2.280059027492552\n","Epoch 9, Loss: 2.2654078564697637\n","LR: 0.001, Hidden Size: 16, Epochs: 10, Dropout: 0.7, Accuracy: 8.5%\n","Epoch 0, Loss: 2.945525918165245\n","Epoch 1, Loss: 2.5084614405936017\n","Epoch 2, Loss: 2.479604666469091\n","Epoch 3, Loss: 2.4172482954006744\n","Epoch 4, Loss: 2.3682785976239313\n","Epoch 5, Loss: 2.3628324147005726\n","Epoch 6, Loss: 2.332986754078166\n","Epoch 7, Loss: 2.32037272944785\n","Epoch 8, Loss: 2.2975553006008456\n","Epoch 9, Loss: 2.2862528119058836\n","Epoch 10, Loss: 2.277749071892043\n","Epoch 11, Loss: 2.2735896126967026\n","Epoch 12, Loss: 2.257752357419571\n","Epoch 13, Loss: 2.267333379813603\n","Epoch 14, Loss: 2.249699767445562\n","Epoch 15, Loss: 2.2581961036923537\n","Epoch 16, Loss: 2.2462151382202493\n","Epoch 17, Loss: 2.2423236751765536\n","Epoch 18, Loss: 2.244837552010266\n","Epoch 19, Loss: 2.2369718042232636\n","LR: 0.001, Hidden Size: 16, Epochs: 20, Dropout: 0.3, Accuracy: 11.0%\n"]}],"source":["import itertools\n","\n","learning_rates = [0.01, 0.001, 0.0001]\n","hidden_sizes = [16, 32, 64]\n","num_epochs = [10, 20, 30]\n","dropout_rates = [0.3, 0.5, 0.7]\n","\n","results = []\n","\n","for lr, hidden_size, epochs, dropout_rate in itertools.product(learning_rates, hidden_sizes, num_epochs, dropout_rates):\n","  model = GCN(in_feats=1, hidden_size=hidden_size, num_classes=10, dropout_rate=dropout_rate)\n","  train(model, trainset, epochs=epochs, learning_rate=lr)\n","  accuracy = evaluate(model, testset)\n","  results.append((lr, hidden_size, epochs, dropout_rate, accuracy))\n","  print(f\"LR: {lr}, Hidden Size: {hidden_size}, Epochs: {epochs}, Dropout: {dropout_rate}, Accuracy: {accuracy}%\")\n","\n","\n","best_hyperparams = max(results, key=lambda x: x[-1])\n","print(\"Best Hyperparameters:\", best_hyperparams)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6eG_cZEDtP5u"},"outputs":[],"source":["def evaluate(model, testset):\n","  model.eval()\n","  dev_loss = 0\n","  predictions = []\n","  true_labels = []\n","  loss_criterion = torch.nn.CrossEntropyLoss()\n","  with torch.no_grad():\n","    for graph, label in testset:\n","      features = graph.ndata['b_factor'].float().unsqueeze(1)\n","      preds = model(graph, features)\n","      loss = loss_criterion(preds, label.view(-1))\n","      dev_loss += loss.item()\n","\n","      _, pred_labels = torch.max(preds.data, 1)\n","      predictions.extend(pred_labels.numpy())\n","      true_labels.append(label.item())\n","  avg_dev_loss = dev_loss / len(testset)\n","  return avg_dev_loss, predictions, true_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CTAaYSdItN5q"},"outputs":[],"source":["def train(model, trainset, epochs, learning_rate):\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","  loss_criterion = torch.nn.CrossEntropyLoss()\n","  for epoch in range(1, epochs + 1):\n","    model.train()\n","    train_loss = 0\n","    train_predictions = []\n","    train_true_labels = []\n","    for graph, label in trainset:\n","      features = graph.ndata['b_factor'].float().unsqueeze(1)\n","      optimizer.zero_grad()\n","      preds = model(graph, features)\n","      loss = loss_criterion(preds, label.view(-1))\n","      train_loss += loss.item()\n","      _, pred_labels = torch.max(preds.data, 1)\n","      train_predictions.extend(pred_labels.numpy())\n","      train_true_labels.append(label.item())\n","      loss.backward()\n","      optimizer.step()\n","    avg_train_loss = train_loss / len(trainset)\n","    print(f\"Epoch {epoch}/{epochs}, Train Loss: {avg_train_loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I1SgU8EMtZiM","outputId":"a233a412-26d0-4bb0-e8ba-d497a28cf9fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Train Loss: 2.7611734471971787\n","Epoch 2/10, Train Loss: 2.3047965154014434\n","Epoch 3/10, Train Loss: 2.3100367403568183\n","Epoch 4/10, Train Loss: 2.30850056598061\n","Epoch 5/10, Train Loss: 2.309676281379578\n","Epoch 6/10, Train Loss: 2.3215335145928804\n","Epoch 7/10, Train Loss: 2.304690524300836\n","Epoch 8/10, Train Loss: 2.2957751179548134\n","Epoch 9/10, Train Loss: 2.2980519046162007\n","Epoch 10/10, Train Loss: 2.3012732164304057\n","LR: 0.01, Hidden Size: 16, Epochs: 10, Dropout: 0.3, Dev Loss: 2.336989666223526, Accuracy: 9.0%\n","Epoch 1/10, Train Loss: 2.541472768518411\n","Epoch 2/10, Train Loss: 2.298008372983837\n","Epoch 3/10, Train Loss: 2.3034818459274176\n","Epoch 4/10, Train Loss: 2.3224698025779915\n","Epoch 5/10, Train Loss: 2.295512059502733\n","Epoch 6/10, Train Loss: 2.2880758003035284\n","Epoch 7/10, Train Loss: 2.285045603612312\n","Epoch 8/10, Train Loss: 2.288876989655626\n","Epoch 9/10, Train Loss: 2.2805626732962474\n","Epoch 10/10, Train Loss: 2.280113892522372\n","LR: 0.01, Hidden Size: 16, Epochs: 10, Dropout: 0.5, Dev Loss: 2.357394051551819, Accuracy: 10.0%\n","Epoch 1/10, Train Loss: 2.6135630403507624\n","Epoch 2/10, Train Loss: 2.3109593950118636\n","Epoch 3/10, Train Loss: 2.3034244488952753\n","Epoch 4/10, Train Loss: 2.2994502009006967\n","Epoch 5/10, Train Loss: 2.2955018762209662\n","Epoch 6/10, Train Loss: 2.2848131708931505\n","Epoch 7/10, Train Loss: 2.295134961007532\n","Epoch 8/10, Train Loss: 2.2796225663190497\n","Epoch 9/10, Train Loss: 2.28844907043273\n","Epoch 10/10, Train Loss: 2.2870686964731766\n","LR: 0.01, Hidden Size: 16, Epochs: 10, Dropout: 0.7, Dev Loss: 2.3389866590499877, Accuracy: 8.0%\n","Epoch 1/20, Train Loss: 2.604931290884663\n","Epoch 2/20, Train Loss: 2.3074763469528734\n","Epoch 3/20, Train Loss: 2.312882464033619\n","Epoch 4/20, Train Loss: 2.304061367995757\n","Epoch 5/20, Train Loss: 2.300065866762534\n","Epoch 6/20, Train Loss: 2.299288087694866\n","Epoch 7/20, Train Loss: 2.297130789374349\n","Epoch 8/20, Train Loss: 2.2920971533708405\n","Epoch 9/20, Train Loss: 2.2858790413927017\n","Epoch 10/20, Train Loss: 2.2792002935158577\n","Epoch 11/20, Train Loss: 2.281294564406077\n","Epoch 12/20, Train Loss: 2.2789281984618435\n","Epoch 13/20, Train Loss: 2.2907074158054246\n","Epoch 14/20, Train Loss: 2.2827253485084475\n","Epoch 15/20, Train Loss: 2.2797221669875887\n","Epoch 16/20, Train Loss: 2.278447109505646\n","Epoch 17/20, Train Loss: 2.2806766486556307\n","Epoch 18/20, Train Loss: 2.2709170847309563\n","Epoch 19/20, Train Loss: 2.2768887926761368\n","Epoch 20/20, Train Loss: 2.2658674621716477\n","LR: 0.01, Hidden Size: 16, Epochs: 20, Dropout: 0.3, Dev Loss: 2.3497046184539796, Accuracy: 8.0%\n","Epoch 1/20, Train Loss: 2.360436595061369\n","Epoch 2/20, Train Loss: 2.2975745096541287\n","Epoch 3/20, Train Loss: 2.288363900325053\n","Epoch 4/20, Train Loss: 2.2852642088008106\n","Epoch 5/20, Train Loss: 2.2789696747049653\n","Epoch 6/20, Train Loss: 2.2875419592200066\n","Epoch 7/20, Train Loss: 2.2850497430727295\n","Epoch 8/20, Train Loss: 2.2790774874818656\n","Epoch 9/20, Train Loss: 2.28375467837305\n","Epoch 10/20, Train Loss: 2.286571067750902\n","Epoch 11/20, Train Loss: 2.2762991112276425\n","Epoch 12/20, Train Loss: 2.2729178372779884\n","Epoch 13/20, Train Loss: 2.278102743296994\n","Epoch 14/20, Train Loss: 2.2644448679192624\n","Epoch 15/20, Train Loss: 2.2750377442155565\n","Epoch 16/20, Train Loss: 2.257467205288416\n","Epoch 17/20, Train Loss: 2.272274784501035\n","Epoch 18/20, Train Loss: 2.273414187413409\n","Epoch 19/20, Train Loss: 2.259144900249957\n","Epoch 20/20, Train Loss: 2.2774574891815806\n","LR: 0.01, Hidden Size: 16, Epochs: 20, Dropout: 0.5, Dev Loss: 2.345216567516327, Accuracy: 8.0%\n","Epoch 1/20, Train Loss: 2.8174724682119856\n","Epoch 2/20, Train Loss: 2.310346569632528\n","Epoch 3/20, Train Loss: 2.3067648041815985\n","Epoch 4/20, Train Loss: 2.3126153765166912\n","Epoch 5/20, Train Loss: 2.3143157785697688\n","Epoch 6/20, Train Loss: 2.306853841868857\n","Epoch 7/20, Train Loss: 2.298073884613233\n","Epoch 8/20, Train Loss: 2.2919063273079714\n","Epoch 9/20, Train Loss: 2.295642728196051\n","Epoch 10/20, Train Loss: 2.29029918247297\n","Epoch 11/20, Train Loss: 2.2887981853688273\n","Epoch 12/20, Train Loss: 2.2853865385951853\n","Epoch 13/20, Train Loss: 2.2900900471778143\n","Epoch 14/20, Train Loss: 2.2783669325193965\n","Epoch 15/20, Train Loss: 2.2832991567470673\n","Epoch 16/20, Train Loss: 2.2849102458708868\n","Epoch 17/20, Train Loss: 2.2884767408807174\n","Epoch 18/20, Train Loss: 2.2957927054331115\n","Epoch 19/20, Train Loss: 2.2847211428901604\n","Epoch 20/20, Train Loss: 2.2812568091210865\n","LR: 0.01, Hidden Size: 16, Epochs: 20, Dropout: 0.7, Dev Loss: 2.347075058221817, Accuracy: 9.0%\n","Epoch 1/30, Train Loss: 2.694516543158885\n","Epoch 2/30, Train Loss: 2.302385388460374\n","Epoch 3/30, Train Loss: 2.3075747604955708\n"]}],"source":["import itertools\n","learning_rates = [0.01, 0.001, 0.0001]\n","hidden_sizes = [16, 32, 64]\n","num_epochs = [10, 20, 30]\n","dropout_rates = [0.3, 0.5, 0.7]\n","results = []\n","for lr, hidden_size, epochs, dropout_rate in itertools.product(learning_rates, hidden_sizes, num_epochs, dropout_rates):\n","    model = GCN(in_feats=1, hidden_size=hidden_size, num_classes=10, dropout_rate=dropout_rate)\n","    train(model, trainset, epochs=epochs, learning_rate=lr)\n","    dev_loss, predictions, true_labels = evaluate(model, testset)\n","    accuracy = sum([pred == true for pred, true in zip(predictions, true_labels)]) / len(true_labels) * 100\n","    results.append((lr, hidden_size, epochs, dropout_rate, accuracy))\n","    print(f\"LR: {lr}, Hidden Size: {hidden_size}, Epochs: {epochs}, Dropout: {dropout_rate}, Dev Loss: {dev_loss}, Accuracy: {accuracy}%\")\n","best_hyperparams = max(results, key=lambda x: x[-1])\n","print(\"Best Hyperparameters:\", best_hyperparams)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}